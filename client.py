"""
MCP å®¢æˆ·ç«¯ - ä¸ Ollama æœ¬åœ°å¤§æ¨¡å‹å’Œ MCP æœåŠ¡å™¨äº¤äº’
ä½¿ç”¨è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç”¨æˆ·æ•°æ®åº“
"""

import asyncio
import json
from typing import Dict, Any
from datetime import datetime
import os
from pathlib import Path

from openai import OpenAI
from openai.types.chat import ChatCompletionMessageParam

# å¯¼å…¥ MCP å®¢æˆ·ç«¯åº“
from mcp.client.stdio import stdio_client, StdioServerParameters
from mcp.client.session import ClientSession
from mcp.types import TextContent

# 1. é…ç½®
OLLAMA_MODEL = "qwen3:14b"
OLLAMA_BASE_URL = "http://localhost:11434/v1"

# åˆ›å»º Ollama OpenAI å…¼å®¹å®¢æˆ·ç«¯
client = OpenAI(
    api_key="ollama",  # Ollama ä¸éœ€è¦çœŸå®çš„ API KEY
    base_url=OLLAMA_BASE_URL,
)


async def call_mcp_tool(tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
    """
    è°ƒç”¨ MCP æœåŠ¡å™¨çš„å·¥å…·

    Args:
        tool_name: å·¥å…·åç§°
        arguments: å·¥å…·å‚æ•°

    Returns:
        å·¥å…·æ‰§è¡Œç»“æœ
    """
    import subprocess
    
    # é…ç½® MCP æœåŠ¡å™¨å‚æ•°
    server_params = StdioServerParameters(
        command="python",
        args=["mcp_server.py"],
    )

    try:
        # ä½¿ç”¨ async with ç¡®ä¿èµ„æºæ­£ç¡®ç®¡ç†
        async with stdio_client(server_params) as (read_stream, write_stream):
            async with ClientSession(read_stream, write_stream) as session:
                # åˆå§‹åŒ–è¿æ¥
                await session.initialize()

                # è°ƒç”¨å·¥å…·
                result = await session.call_tool(tool_name, arguments)

                # æå–å·¥å…·è¿”å›çš„å†…å®¹
                if hasattr(result, "content") and result.content:
                    # è·å–ç¬¬ä¸€ä¸ªæ–‡æœ¬å†…å®¹
                    for content_item in result.content:
                        if isinstance(content_item, TextContent):
                            content_text = content_item.text
                            # å°è¯•è§£æ JSON
                            try:
                                return json.loads(content_text)
                            except:
                                return {"result": content_text}
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬å†…å®¹
                    return {"result": str(result)}
                else:
                    return {"result": str(result)}

    except Exception as e:
        print(f"è°ƒç”¨å·¥å…·æ—¶å‡ºé”™: {e}")
        import traceback

        traceback.print_exc()
        return {"error": str(e)}


class NaturalLanguageQuery:
    """è‡ªç„¶è¯­è¨€æŸ¥è¯¢å¤„ç†å™¨"""

    def __init__(self):
        self.last_log_position = 0  # è®°å½•ä¸Šæ¬¡è¯»å–æ—¥å¿—æ–‡ä»¶çš„ä½ç½®

    def _read_and_display_server_logs(self):
        """è¯»å–å¹¶æ˜¾ç¤º MCP æœåŠ¡å™¨æ—¥å¿—æ–‡ä»¶ä¸­çš„æ–°å†…å®¹"""
        log_file = Path("mcp_server.log")
        
        if not log_file.exists():
            return
        
        try:
            file_size = log_file.stat().st_size
            
            # å¦‚æœæ–‡ä»¶æœ‰æ–°å†…å®¹ï¼Œè¯»å–æ–°å¢éƒ¨åˆ†
            if file_size > self.last_log_position:
                with open(log_file, "r", encoding="utf-8") as f:
                    f.seek(self.last_log_position)
                    new_content = f.read()
                    
                    if new_content.strip():
                        print(f"\n[MCP æœåŠ¡å™¨æ—¥å¿—]:")
                        print(new_content, end="")
                
                self.last_log_position = file_size
        except Exception as e:
            pass  # é™é»˜å¤„ç†è¯»å–å¤±è´¥

    def _convert_arguments(self, tool_name: str, arguments: dict) -> dict:
        """
        è½¬æ¢å’ŒéªŒè¯å·¥å…·å‚æ•°çš„ç±»å‹

        Args:
            tool_name: å·¥å…·åç§°
            arguments: åŸå§‹å‚æ•°å­—å…¸

        Returns:
            è½¬æ¢åçš„å‚æ•°å­—å…¸
        """
        converted = {}

        if tool_name == "query_users":
            # ç§»é™¤ None å€¼å’Œä¸å­˜åœ¨çš„å‚æ•°
            for key in ["name", "min_age", "max_age", "age_greater_than", "age_less_than", "email_contains"]:
                if key in arguments and arguments[key] is not None:
                    if key in ["min_age", "max_age", "age_greater_than", "age_less_than"]:
                        # è½¬æ¢ä¸ºæ•´æ•°
                        try:
                            converted[key] = int(arguments[key])
                        except (ValueError, TypeError):
                            print(f"[è­¦å‘Š] {key} æ— æ³•è½¬æ¢ä¸ºæ•´æ•°ï¼Œè·³è¿‡è¯¥å‚æ•°")
                    else:
                        # name å’Œ email_contains ä¿æŒä¸ºå­—ç¬¦ä¸²
                        converted[key] = str(arguments[key])

        elif tool_name == "get_user_by_id":
            # user_id å¿…é¡»æ˜¯æ•´æ•°
            if "user_id" in arguments:
                try:
                    converted["user_id"] = int(arguments["user_id"])
                except (ValueError, TypeError):
                    print(f"[è­¦å‘Š] user_id æ— æ³•è½¬æ¢ä¸ºæ•´æ•°")
                    converted["user_id"] = arguments["user_id"]

        return converted if converted else arguments

    async def _get_tool_decision(self, question: str) -> str:
        """
        è®© LLM å†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ä»¥åŠå¦‚ä½•è°ƒç”¨

        Args:
            question: ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€é—®é¢˜

        Returns:
            åŒ…å«å·¥å…·è°ƒç”¨å†³ç­–çš„ JSON å­—ç¬¦ä¸²
        """
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æŸ¥è¯¢ç”¨æˆ·æ•°æ®åº“ã€‚
        
å¯ç”¨çš„å·¥å…·ï¼š
1. query_users - æ ¹æ®æ¡ä»¶æŸ¥è¯¢å¤šä¸ªç”¨æˆ·
   å‚æ•°ï¼š
   - name: æŒ‰å§“åæ¨¡ç³ŠæŸ¥è¯¢ï¼ˆå¯é€‰ï¼Œå­—ç¬¦ä¸²ç±»å‹ï¼‰
   - min_age: æœ€å°å¹´é¾„ï¼ŒåŒ…å«è¾¹ç•Œï¼Œå³ >= min_ageï¼ˆå¯é€‰ï¼Œæ•´æ•°ç±»å‹ï¼Œå¦‚30ï¼‰
   - max_age: æœ€å¤§å¹´é¾„ï¼ŒåŒ…å«è¾¹ç•Œï¼Œå³ <= max_ageï¼ˆå¯é€‰ï¼Œæ•´æ•°ç±»å‹ï¼Œå¦‚35ï¼‰
   - age_greater_than: å¹´é¾„å¤§äºï¼Œä¸åŒ…å«è¾¹ç•Œï¼Œå³ > age_greater_thanï¼ˆå¯é€‰ï¼Œæ•´æ•°ç±»å‹ï¼Œå¦‚30ï¼‰
   - age_less_than: å¹´é¾„å°äºï¼Œä¸åŒ…å«è¾¹ç•Œï¼Œå³ < age_less_thanï¼ˆå¯é€‰ï¼Œæ•´æ•°ç±»å‹ï¼Œå¦‚25ï¼‰
   - email_contains: é‚®ç®±åŒ…å«çš„å­—ç¬¦ä¸²ï¼ˆå¯é€‰ï¼Œå­—ç¬¦ä¸²ç±»å‹ï¼‰

2. get_user_by_id - æ ¹æ®IDè·å–å•ä¸ªç”¨æˆ·
   å‚æ•°ï¼š
   - user_id: ç”¨æˆ·IDï¼ˆå¿…éœ€ï¼Œæ•´æ•°ç±»å‹ï¼‰

å½“ç”¨æˆ·è¯¢é—®ç”¨æˆ·ä¿¡æ¯æ—¶ï¼Œä½ åº”è¯¥ï¼š
1. åˆ†æç”¨æˆ·çš„é—®é¢˜ï¼Œåˆ¤æ–­éœ€è¦å“ªä¸ªå·¥å…·
2. æå–ç›¸å…³å‚æ•°ï¼Œæ³¨æ„å‚æ•°ç±»å‹å¿…é¡»æ­£ç¡®ï¼ˆæ•´æ•°ç±»å‹çš„å­—æ®µä¸è¦ä½¿ç”¨å­—ç¬¦ä¸²ï¼‰
3. ä»¥ä¸¥æ ¼çš„ JSON æ ¼å¼è¿”å›å·¥å…·è°ƒç”¨ä¿¡æ¯

JSON æ ¼å¼ï¼š
{
  "tool": "å·¥å…·åç§°",
  "arguments": {
    "name": null,
    "min_age": null,
    "max_age": null,
    "age_greater_than": null,
    "age_less_than": null,
    "email_contains": null
  }
}

é‡è¦æç¤ºï¼š
- æ•´æ•°å‚æ•°ï¼ˆå¦‚ min_ageã€max_ageã€age_greater_thanã€age_less_thanã€user_idï¼‰å¿…é¡»æ˜¯æ•°å­—ï¼Œä¸è¦åŠ å¼•å·
- å­—ç¬¦ä¸²å‚æ•°ï¼ˆå¦‚ nameã€email_containsï¼‰è¦åŠ å¼•å·
- ä¸éœ€è¦çš„å‚æ•°è®¾ä¸º null
- ä¸è¦è¿”å›å¤šä½™çš„å­—æ®µ

å¦‚æœé—®é¢˜ä¸éœ€è¦æŸ¥è¯¢æ•°æ®åº“ï¼Œç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦è¿”å› JSONã€‚
"""

        messages: list[ChatCompletionMessageParam] = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question},
        ]

        # æ‰“å°å®Œæ•´çš„æç¤ºè¯
        print(f"\n" + "=" * 70)
        print(f"[å‘ç»™ LLM çš„åŸå§‹è¯·æ±‚æ–‡æœ¬ #1 - å·¥å…·å†³ç­–]")
        print(f"=" * 70)
        print(f"System: {system_prompt}")
        print(f"\nUser: {question}")
        print(f"=" * 70)

        response = client.chat.completions.create(
            model=OLLAMA_MODEL,
            messages=messages,
            temperature=0,
        )

        llm_response = response.choices[0].message.content or ""

        # æ‰“å° LLM çš„åŸå§‹å›ç­”
        print(f"\nğŸ¤– [æ¥è‡ª LLM çš„åŸå§‹å“åº”æ–‡æœ¬ #1]:")
        print(f"{llm_response}")
        print(f"=" * 70)

        return llm_response

    async def query(self, question: str) -> str:
        """
        å¤„ç†è‡ªç„¶è¯­è¨€æŸ¥è¯¢

        Args:
            question: ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€é—®é¢˜

        Returns:
            AI ç”Ÿæˆçš„å›ç­”
        """
        try:
            start_time = datetime.now()
            print(f"[{start_time.strftime('%H:%M:%S')}] å¼€å§‹å¤„ç†é—®é¢˜")

            # ç¬¬ä¸€æ­¥ï¼šè®© LLM å†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
            decision_start = datetime.now()
            print(f"[{decision_start.strftime('%H:%M:%S')}] å‘ LLM å‘é€å·¥å…·å†³ç­–è¯·æ±‚")

            tool_decision_str = await self._get_tool_decision(question)

            decision_end = datetime.now()
            decision_time = (decision_end - decision_start).total_seconds()
            print(f"[{decision_end.strftime('%H:%M:%S')}] LLM å†³ç­–å®Œæˆ (è€—æ—¶: {decision_time:.2f}ç§’)")

            # å°è¯•è§£æ JSON
            try:
                tool_decision = json.loads(tool_decision_str)

                # æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨ä¿¡æ¯
                if "tool" in tool_decision and "arguments" in tool_decision:
                    tool_name = tool_decision["tool"]
                    arguments = tool_decision["arguments"]

                    # è½¬æ¢å‚æ•°ç±»å‹
                    arguments = self._convert_arguments(tool_name, arguments)

                    print(f"\n[å·¥å…·è°ƒç”¨] {tool_name}")
                    print(f"[å·¥å…·å‚æ•°] {arguments}")

                    # è°ƒç”¨ MCP å·¥å…·
                    tool_start = datetime.now()
                    print(f"[{tool_start.strftime('%H:%M:%S')}] å¼€å§‹è°ƒç”¨ MCP å·¥å…·")

                    tool_result = await call_mcp_tool(tool_name, arguments)

                    tool_end = datetime.now()
                    tool_time = (tool_end - tool_start).total_seconds()
                    print(f"[{tool_end.strftime('%H:%M:%S')}] å·¥å…·è°ƒç”¨å®Œæˆ (è€—æ—¶: {tool_time:.2f}ç§’)")
                    print(f"[å·¥å…·ç»“æœ] {tool_result}")
                    
                    # æ˜¾ç¤º MCP æœåŠ¡å™¨çš„æ—¥å¿—
                    self._read_and_display_server_logs()

                    # ç¬¬äºŒæ­¥ï¼šè®© LLM æ ¹æ®å·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”
                    final_start = datetime.now()
                    print(f"\n[{final_start.strftime('%H:%M:%S')}] å‘ LLM è¯·æ±‚ç”Ÿæˆæœ€ç»ˆå›ç­”")

                    system_content = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ ¹æ®å·¥å…·è¿”å›çš„æ•°æ®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
- ä½¿ç”¨ç®€æ´æ¸…æ™°çš„ä¸­æ–‡å›ç­”
- ä¸è¦ä½¿ç”¨ä»»ä½• markdown è¯­æ³•ï¼ˆå¦‚ **åŠ ç²—**ã€## æ ‡é¢˜ã€- åˆ—è¡¨ç­‰ï¼‰
- ä¸è¦è¾“å‡ºä»£ç å—æˆ–ä»£ç æ®µ
- ç›´æ¥è¾“å‡ºçº¯æ–‡æœ¬ç­”æ¡ˆï¼Œä¾¿äºç»ˆç«¯æ˜¾ç¤º
- å¦‚æœéœ€è¦åˆ—ä¸¾å†…å®¹ï¼Œä½¿ç”¨ "1ã€2ã€3" æˆ–ä½¿ç”¨ç®€å•çš„æ–‡å­—æè¿°"""
                    user_content = (
                        f"é—®é¢˜: {question}\nå·¥å…·ç»“æœ: {json.dumps(tool_result, ensure_ascii=False, indent=2)}"
                    )

                    final_messages: list[ChatCompletionMessageParam] = [
                        {
                            "role": "system",
                            "content": system_content,
                        },
                        {
                            "role": "user",
                            "content": user_content,
                        },
                    ]

                    # æ‰“å°å®Œæ•´çš„æç¤ºè¯
                    print(f"\n" + "=" * 70)
                    print(f"[å‘ç»™ LLM çš„åŸå§‹è¯·æ±‚æ–‡æœ¬ #2 - æœ€ç»ˆå›ç­”ç”Ÿæˆ]")
                    print(f"=" * 70)
                    print(f"System: {system_content}")
                    print(f"\nUser: {user_content}")
                    print(f"=" * 70)

                    final_response = client.chat.completions.create(
                        model=OLLAMA_MODEL,
                        messages=final_messages,
                        temperature=0.2,
                    )

                    final_end = datetime.now()
                    final_time = (final_end - final_start).total_seconds()
                    total_time = (final_end - start_time).total_seconds()

                    llm_final_answer = final_response.choices[0].message.content or ""

                    print(f"[{final_end.strftime('%H:%M:%S')}] æœ€ç»ˆå›ç­”ç”Ÿæˆå®Œæˆ (è€—æ—¶: {final_time:.2f}ç§’)")
                    print(f"[æ€»è€—æ—¶] {total_time:.2f}ç§’")

                    return llm_final_answer

            except json.JSONDecodeError:
                # ä¸æ˜¯æœ‰æ•ˆçš„ JSONï¼Œç›´æ¥è¿”å› LLM çš„å›ç­”
                pass

            # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å› LLM çš„å›ç­”
            total_time = (datetime.now() - start_time).total_seconds()
            print(f"[æ€»è€—æ—¶] {total_time:.2f}ç§’")
            return tool_decision_str

        except Exception as e:
            return f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"


async def main():
    """ä¸»å‡½æ•°"""
    print("æ™ºèƒ½ç”¨æˆ·æŸ¥è¯¢åŠ©æ‰‹ (MCP + Ollama æœ¬åœ°å¤§æ¨¡å‹)")
    print(f"å·²è¿æ¥åˆ° Ollama æœ¬åœ°æ¨¡å‹ ({OLLAMA_MODEL})")
    print("è¾“å…¥é—®é¢˜å¼€å§‹æŸ¥è¯¢ï¼Œè¾“å…¥ 'exit' é€€å‡º")
    print("-" * 70)

    # åˆ›å»ºæŸ¥è¯¢å¤„ç†å™¨
    query_processor = NaturalLanguageQuery()

    while True:
        question = input("\nğŸ‘¤ ä½ : ")
        if question.lower() == "exit":
            break

        question_time = datetime.now()
        print(f"[{question_time.strftime('%H:%M:%S')}] æ”¶åˆ°é—®é¢˜")

        try:
            # å¤„ç†æŸ¥è¯¢
            answer = await query_processor.query(question)
            print(f"\nğŸ¤– AI å‘ç»™ç”¨æˆ·çš„å›ç­”: {answer}")
        except Exception as e:
            print(f"\né”™è¯¯: {str(e)}")
            import traceback

            traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
